{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea9e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A pytorch implementation of a transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "from TransformLib.Transformer import Decoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b95583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size:int,d_model:int,n_layers:int,h:int,d_ff:int,max_seq_len:int = 512):\n",
    "        super().__init__()\n",
    "        self.decode = Decoder(vocab_size,d_model,n_layers,h,d_ff,max_seq_len)\n",
    "        self.output = nn.Linear(d_model,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,x,target_mask):\n",
    "        x = self.decode(x,None,None,target_mask)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ca94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<CLS>': 0, '<SEP>': 1, '<PAD>': 2, 'date': 3, 'changing': 4, 'information': 5, 'applying': 6, 'unsuccessful': 7, 'issued': 8, '?': 9, 'increased': 10, 'printed': 11, 'works': 12, 'working': 13, 'actual': 14, 'involving': 15, 'from': 16, 'time': 17, 'unable': 18, 'facing': 19, 'education': 20, 'shop': 21, 'interest': 22, 'because': 23, 'corrected': 24, 'password': 25, 'system': 26, 'through': 27, 'otp': 28, 'that': 29, 'charge': 30, 'bill': 31, 'fund': 32, 'updated': 33, 'previously': 34, 'week': 35, '\"': 36, 'exact': 37, ',': 38, 'via': 39, 'how': 40, 'intermediaries': 41, 'opened': 42, 'mobile': 43, 'been': 44, 'withdrawn': 45, 'correction': 46, 'loss': 47, 'provided': 48, 'get': 49, 'passbook': 50, 'pos': 51, 'process': 52, 'not': 53, 'processed': 54, 'insufficient': 55, 'purchasing': 56, 'correct': 57, 'started': 58, 'cause': 59, 'check': 60, 'able': 61, 'daily': 62, 'although': 63, 'closing': 64, 'has': 65, 'withdrew': 66, 'yesterday': 67, 'updating': 68, '7': 69, 'into': 70, 'bank': 71, 'necessary': 72, 'last': 73, 'add': 74, 'approved': 75, 'make': 76, 'connection': 77, 'yet': 78, 'new': 79, 'want': 80, 'registered': 81, 'day': 82, 'emi': 83, 'as': 84, 'credited': 85, 'settlement': 86, 'instead': 87, 'do': 88, 'digital': 89, 'old': 90, 'declined': 91, 'details': 92, 'deducted': 93, 'by': 94, 'days': 95, 'detected': 96, 'was': 97, 'block': 98, 'of': 99, 'sufficient': 100, 'for': 101, 'other': 102, 'during': 103, 'can': 104, 'booking': 105, 'paid': 106, 'pin': 107, 'gateway': 108, 'send': 109, 'on': 110, 'shopping': 111, 'merchants': 112, 'the': 113, 'irctc': 114, 'any': 115, 'which': 116, 'personal': 117, 'changed': 118, 'things': 119, 'fullfill': 120, 'could': 121, 'collected': 122, 'find': 123, 'regarding': 124, 'loan': 125, '3': 126, 'status': 127, 'found': 128, 'activation': 129, 'there': 130, 'authentication': 131, 'transactions': 132, 'shows': 133, 'one': 134, 'internet': 135, 'an': 136, 'user': 137, 'opening': 138, 'atm': 139, 'today': 140, '.': 141, 'related': 142, 'online': 143, 'message': 144, 'mentioned': 145, 'linking': 146, 'so': 147, 'address': 148, '-': 149, 'number': 150, 'all': 151, 'reason': 152, 'be': 153, 'are': 154, 'apply': 155, 'with': 156, 'some': 157, 'security': 158, 'deduced': 159, 'completed': 160, 'correcting': 161, 'middle': 162, 'i': 163, 'got': 164, 'again': 165, 'debited': 166, 'gave': 167, 'copy': 168, 'received': 169, 'you': 170, 'incorrect': 171, 'entered': 172, 'payment': 173, 'gone': 174, 'attached': 175, 'documents': 176, 'limits': 177, 'issuing': 178, 'this': 179, 'document': 180, 'reset': 181, 'wrong': 182, 'reveals': 183, '/': 184, 'sms': 185, 'also': 186, 'end': 187, 'mall': 188, 'transfer': 189, 'local': 190, 'failure': 191, 'done': 192, 'deduction': 193, 'valid': 194, 'given': 195, 'along': 196, 'to': 197, 'whenever': 198, 'many': 199, 'debit': 200, 'below': 201, 'made': 202, 'use': 203, 'increasing': 204, 'withdrawing': 205, 'in': 206, 'making': 207, 'book': 208, 'complete': 209, 'poor': 210, 'your': 211, 'it': 212, 'why': 213, 'refunded': 214, 'applied': 215, 'due': 216, 'banking': 217, 'but': 218, 'criteria': 219, 'recognised': 220, 'large': 221, 'joint': 222, 'calculated': 223, 'forgot': 224, 'name': 225, 'settled': 226, 'generate': 227, 'my': 228, 'credit': 229, 'month': 230, 'will': 231, 'size': 232, 'ten': 233, 'cascading': 234, 'at': 235, 'denied': 236, 'pending': 237, 'failed': 238, 'balance': 239, 'account': 240, 'or': 241, 'allowed': 242, 'linked': 243, 'a': 244, 'transaction': 245, 'is': 246, 'ago': 247, 'form': 248, 'soon': 249, 'mother': 250, 'father': 251, 'something': 252, 'out': 253, 'another': 254, 'tell': 255, 'conformed': 256, 'till': 257, 'now': 258, 'refund': 259, 'have': 260, 'kolkata': 261, 'card': 262, 'possible': 263, 'side': 264, 'change': 265, 'amount': 266, 'go': 267, 'after': 268, 'birth': 269, 'problem': 270, 'cash': 271, 'where': 272, 'required': 273, 'delays': 274, 'while': 275, 'wrongly': 276, 'money': 277, 'pan': 278, 'shown': 279, 'site': 280, 'mechanical': 281, 'what': 282, 'ticket': 283, 'without': 284, 'error': 285, 'gets': 286, 'successful': 287, 'inquire': 288, 'am': 289, 'machine': 290, 'did': 291, 'complaint': 292, 'and': 293, 'using': 294, 'even': 295, 'detail': 296, 'lost': 297, 'entering': 298, 'please': 299, 'disbursal': 300, 'sorry': 301, 'claim': 302, 'correctly': 303, 'though': 304, 'application': 305, 'when': 306, 'unique': 307}\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "data = df['text'].tolist()\n",
    "\n",
    "wordlist = []\n",
    "\n",
    "for i in data:\n",
    "    words = i.split()\n",
    "    for w in words:\n",
    "        wordlist.append(w.lower())\n",
    "\n",
    "word_indices = dict({\"<CLS>\":0,\"<SEP>\":1,\"<PAD>\":2})\n",
    " \n",
    "index = 3\n",
    "for w in set(wordlist):\n",
    "    word_indices.update({w:index})\n",
    "    index+=1\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87818e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_indices)\n",
    "d_model = 32\n",
    "n_layers = 8\n",
    "h = 8\n",
    "d_ff = 128\n",
    "max_len = 64\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "lm = LanguageModel(vocab_size,d_model,n_layers,h,d_ff,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f8c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 163, 215, 101, 244, 229, 262, 73, 230, 218, 163, 291, 53, 49, 29, 134, 257, 258, 141, 63, 163, 120, 151, 113, 219, 273, 101, 6, 229, 262, 141, 1], [0, 163, 42, 244, 79, 240, 206, 211, 71, 218, 306, 163, 164, 228, 50, 163, 128, 29, 228, 162, 225, 246, 276, 11, 110, 212, 141, 1], [0, 163, 215, 101, 200, 241, 229, 262, 306, 163, 42, 228, 240, 206, 211, 71, 73, 230, 218, 163, 291, 53, 49, 115, 257, 258, 141, 1], [0, 257, 258, 163, 291, 53, 49, 115, 200, 241, 229, 262, 116, 163, 215, 73, 230, 206, 211, 71, 141, 1], [0, 163, 167, 136, 305, 101, 146, 307, 150, 156, 228, 71, 240, 73, 230, 218, 257, 258, 212, 246, 53, 33, 141, 1], [0, 163, 80, 197, 265, 113, 43, 150, 29, 97, 34, 243, 156, 228, 240, 293, 80, 197, 74, 228, 79, 150, 141, 1], [0, 163, 297, 228, 200, 241, 229, 262, 67, 141, 147, 163, 80, 197, 98, 228, 200, 241, 229, 262, 141, 113, 142, 5, 246, 195, 201, 141, 1], [0, 299, 98, 228, 200, 241, 229, 262, 23, 163, 297, 228, 200, 241, 229, 262, 67, 141, 151, 113, 142, 5, 246, 195, 201, 141, 1], [0, 163, 167, 136, 305, 101, 146, 228, 43, 150, 156, 228, 240, 73, 35, 218, 257, 258, 163, 88, 53, 49, 115, 5, 124, 29, 141, 1], [0, 163, 167, 136, 305, 101, 146, 228, 43, 150, 156, 228, 240, 73, 230, 218, 163, 291, 53, 49, 115, 185, 142, 197, 228, 240, 110, 81, 113, 150, 141, 1], [0, 163, 215, 101, 244, 79, 240, 206, 211, 71, 218, 306, 163, 164, 228, 50, 163, 128, 29, 113, 225, 99, 228, 251, 246, 276, 11, 110, 212, 141, 1], [0, 163, 289, 53, 61, 197, 76, 173, 27, 228, 200, 241, 229, 262, 141, 136, 285, 144, 97, 279, 29, 36, 179, 262, 246, 53, 220, 36, 141, 1], [0, 103, 173, 27, 200, 262, 212, 133, 173, 191, 141, 136, 285, 144, 97, 279, 29, 36, 179, 262, 246, 53, 220, 36, 141, 1], [0, 163, 215, 101, 244, 79, 240, 206, 211, 71, 218, 306, 163, 164, 228, 50, 163, 128, 29, 228, 3, 99, 269, 246, 276, 11, 110, 113, 50, 141, 1], [0, 163, 80, 197, 74, 228, 278, 262, 150, 197, 228, 240, 147, 29, 163, 104, 76, 221, 245, 94, 294, 212, 141, 1], [0, 163, 76, 244, 173, 27, 228, 200, 262, 235, 244, 111, 188, 141, 113, 173, 127, 133, 191, 99, 173, 218, 239, 246, 159, 16, 228, 240, 141, 1], [0, 113, 173, 246, 53, 160, 218, 239, 246, 159, 16, 228, 240, 306, 163, 76, 244, 173, 235, 244, 111, 188, 141, 1], [0, 163, 167, 136, 305, 101, 68, 228, 117, 92, 110, 228, 240, 218, 212, 246, 53, 192, 257, 258, 141, 163, 186, 175, 113, 72, 176, 156, 113, 305, 141, 1], [0, 163, 215, 101, 138, 244, 222, 240, 156, 228, 250, 73, 230, 218, 306, 163, 164, 228, 240, 296, 163, 128, 29, 212, 246, 244, 117, 240, 141, 1], [0, 163, 215, 101, 138, 244, 117, 240, 73, 230, 218, 306, 163, 164, 228, 240, 296, 163, 128, 29, 212, 246, 244, 222, 240, 156, 228, 251, 141, 1], [0, 163, 167, 136, 305, 101, 64, 228, 240, 110, 211, 71, 73, 35, 141, 163, 186, 145, 113, 194, 152, 101, 64, 113, 240, 87, 212, 246, 53, 192, 257, 258, 141, 1], [0, 163, 215, 101, 135, 217, 129, 110, 228, 240, 73, 35, 218, 257, 258, 163, 88, 53, 49, 115, 5, 124, 29, 141, 163, 48, 151, 113, 273, 5, 156, 228, 305, 248, 141, 1], [0, 163, 224, 228, 135, 217, 25, 293, 80, 197, 181, 218, 163, 289, 53, 61, 197, 181, 113, 25, 141, 1], [0, 163, 224, 228, 135, 217, 137, 225, 293, 80, 197, 181, 212, 218, 163, 97, 18, 197, 88, 29, 141, 1], [0, 163, 76, 245, 27, 200, 262, 101, 205, 277, 16, 139, 141, 113, 245, 246, 53, 160, 218, 277, 246, 159, 16, 228, 240, 141, 1], [0, 163, 215, 101, 135, 217, 129, 110, 228, 240, 233, 82, 247, 218, 163, 88, 53, 49, 228, 137, 225, 293, 25, 257, 258, 141, 1], [0, 163, 215, 101, 4, 228, 117, 240, 197, 244, 222, 240, 156, 228, 251, 73, 230, 218, 257, 258, 212, 246, 53, 118, 141, 1], [0, 163, 167, 136, 305, 101, 161, 228, 190, 148, 99, 228, 240, 218, 212, 246, 53, 24, 257, 67, 141, 1], [0, 163, 203, 228, 200, 262, 101, 205, 277, 16, 228, 240, 141, 113, 245, 246, 53, 160, 216, 197, 157, 152, 218, 277, 246, 159, 16, 228, 240, 141, 1], [0, 163, 203, 228, 200, 262, 101, 207, 173, 110, 136, 143, 280, 141, 277, 246, 159, 16, 228, 240, 218, 173, 127, 246, 279, 84, 7, 141, 1], [0, 163, 202, 244, 245, 110, 136, 143, 280, 101, 56, 252, 141, 113, 173, 127, 246, 53, 287, 218, 277, 246, 159, 16, 228, 240, 141, 1], [0, 163, 80, 197, 155, 101, 20, 125, 218, 113, 307, 150, 243, 197, 228, 240, 246, 53, 57, 141, 163, 215, 101, 161, 113, 307, 150, 218, 212, 246, 53, 24, 78, 141, 163, 186, 175, 134, 168, 99, 228, 57, 307, 150, 196, 156, 228, 305, 141, 1], [0, 163, 167, 136, 305, 101, 146, 244, 79, 43, 150, 197, 228, 240, 84, 228, 90, 243, 43, 150, 246, 53, 13, 218, 257, 258, 212, 246, 53, 192, 141, 216, 197, 179, 163, 289, 19, 199, 270, 141, 1], [0, 163, 167, 136, 305, 101, 204, 228, 62, 245, 177, 141, 163, 186, 175, 244, 168, 99, 228, 278, 262, 156, 113, 305, 248, 218, 212, 246, 53, 10, 257, 258, 141, 299, 88, 212, 84, 249, 84, 263, 141, 1], [0, 163, 80, 197, 189, 277, 16, 228, 240, 197, 254, 240, 39, 228, 200, 262, 218, 53, 61, 197, 209, 113, 189, 141, 212, 133, 136, 285, 141, 1], [0, 163, 80, 197, 189, 277, 16, 228, 240, 197, 254, 240, 39, 228, 200, 262, 141, 218, 28, 246, 53, 109, 94, 71, 197, 228, 81, 43, 150, 147, 29, 163, 104, 76, 113, 245, 141, 1], [0, 163, 202, 244, 245, 156, 228, 200, 262, 101, 56, 119, 16, 244, 21, 206, 261, 141, 113, 245, 246, 53, 287, 218, 277, 246, 159, 16, 228, 240, 141, 1], [0, 245, 127, 246, 226, 16, 228, 264, 218, 97, 53, 169, 110, 113, 102, 187, 1], [0, 113, 245, 235, 134, 187, 246, 226, 218, 130, 154, 274, 206, 86, 235, 113, 102, 187, 1], [0, 47, 206, 113, 239, 206, 240, 216, 197, 26, 191, 275, 143, 245, 1], [0, 163, 202, 244, 245, 110, 114, 294, 228, 200, 262, 101, 283, 105, 141, 105, 246, 53, 256, 295, 277, 246, 45, 16, 228, 240, 141, 1], [0, 163, 215, 101, 83, 110, 136, 143, 111, 280, 294, 228, 229, 262, 141, 83, 246, 58, 110, 228, 240, 218, 212, 246, 53, 279, 110, 113, 111, 280, 141, 1], [0, 103, 205, 277, 16, 240, 294, 200, 262, 16, 244, 139, 38, 212, 97, 279, 29, 29, 113, 107, 170, 172, 246, 171, 63, 163, 172, 113, 57, 107, 141, 1], [0, 113, 148, 145, 110, 228, 240, 246, 53, 57, 141, 163, 215, 101, 148, 46, 73, 35, 218, 212, 246, 53, 24, 257, 140, 141, 1], [0, 113, 107, 170, 172, 246, 182, 144, 97, 279, 306, 163, 97, 205, 277, 16, 139, 294, 200, 262, 293, 298, 113, 57, 107, 141, 1], [0, 163, 80, 197, 265, 228, 200, 262, 107, 218, 53, 61, 197, 88, 147, 235, 115, 139, 290, 141, 1], [0, 163, 289, 53, 61, 197, 265, 228, 200, 262, 158, 107, 294, 115, 139, 290, 63, 163, 172, 113, 57, 5, 273, 101, 29, 52, 141, 1], [0, 40, 88, 163, 123, 253, 213, 244, 173, 241, 259, 245, 238, 1], [0, 173, 127, 246, 75, 94, 212, 65, 53, 44, 169, 1], [0, 103, 205, 277, 16, 139, 294, 200, 38, 113, 245, 246, 238, 218, 277, 246, 159, 16, 228, 240, 16, 228, 240, 141, 1], [0, 53, 61, 197, 76, 115, 143, 245, 94, 228, 200, 262, 63, 163, 172, 151, 113, 5, 303, 141, 1], [0, 163, 215, 101, 68, 228, 278, 262, 293, 307, 262, 150, 197, 228, 240, 73, 35, 218, 212, 246, 53, 33, 257, 140, 141, 163, 186, 175, 244, 168, 99, 113, 273, 176, 156, 113, 305, 248, 141, 1], [0, 163, 215, 101, 161, 228, 225, 110, 228, 240, 73, 230, 218, 257, 140, 212, 246, 53, 24, 141, 216, 197, 179, 228, 102, 12, 154, 237, 272, 163, 260, 197, 203, 228, 71, 92, 141, 1], [0, 163, 215, 101, 60, 208, 73, 230, 218, 257, 140, 163, 88, 53, 49, 115, 141, 1], [0, 244, 60, 208, 246, 8, 110, 228, 240, 218, 163, 88, 53, 215, 101, 115, 134, 141, 216, 197, 179, 30, 101, 178, 60, 208, 246, 159, 16, 228, 240, 141, 1], [0, 53, 61, 197, 189, 277, 197, 254, 240, 94, 228, 200, 262, 141, 1], [0, 163, 215, 101, 113, 79, 200, 262, 73, 230, 218, 291, 53, 49, 115, 257, 258, 63, 163, 48, 151, 113, 273, 180, 235, 113, 17, 99, 6, 141, 1], [0, 163, 164, 228, 79, 200, 262, 67, 218, 163, 289, 53, 61, 197, 227, 228, 158, 107, 235, 115, 139, 141, 1], [0, 228, 240, 133, 113, 277, 65, 44, 166, 218, 212, 65, 53, 44, 106, 235, 113, 280, 141, 1], [0, 228, 173, 121, 53, 153, 54, 38, 304, 163, 260, 100, 239, 206, 228, 240, 141, 1], [0, 198, 163, 267, 101, 173, 38, 130, 246, 131, 191, 141, 1], [0, 130, 97, 244, 173, 108, 191, 293, 277, 65, 44, 166, 141, 1], [0, 306, 231, 113, 277, 153, 85, 165, 9, 1], [0, 173, 236, 216, 197, 55, 32, 144, 38, 304, 163, 260, 100, 239, 141, 1], [0, 163, 42, 244, 79, 240, 206, 211, 71, 218, 306, 163, 164, 228, 50, 163, 128, 29, 228, 162, 225, 246, 276, 11, 110, 212, 141, 1], [0, 163, 215, 101, 200, 241, 229, 262, 306, 163, 42, 228, 240, 206, 211, 71, 73, 230, 218, 163, 291, 53, 49, 115, 257, 258, 141, 1], [0, 257, 258, 163, 291, 53, 49, 115, 200, 241, 229, 262, 116, 163, 215, 73, 230, 206, 211, 71, 141, 1], [0, 163, 167, 136, 305, 101, 146, 307, 150, 156, 228, 71, 240, 73, 230, 218, 257, 258, 212, 246, 53, 33, 141, 1], [0, 245, 53, 226, 1], [0, 238, 139, 245, 1], [0, 206, 139, 240, 286, 166, 284, 14, 300, 99, 271, 1], [0, 206, 173, 132, 15, 41, 38, 113, 173, 197, 97, 53, 202, 197, 112, 1], [0, 245, 127, 246, 226, 16, 228, 264, 218, 97, 53, 169, 110, 113, 102, 187, 1], [0, 113, 245, 235, 134, 187, 246, 226, 218, 130, 154, 274, 206, 86, 235, 113, 102, 187, 1], [0, 47, 206, 113, 239, 206, 240, 216, 197, 26, 191, 275, 143, 245, 1], [0, 130, 65, 44, 234, 191, 206, 228, 240, 293, 113, 22, 65, 186, 44, 223, 110, 113, 182, 266, 1], [0, 277, 65, 44, 93, 16, 228, 240, 1], [0, 163, 289, 53, 61, 197, 265, 228, 200, 262, 158, 107, 294, 115, 139, 290, 63, 163, 172, 113, 57, 5, 273, 101, 29, 52, 141, 1], [0, 103, 205, 277, 16, 139, 294, 200, 38, 113, 245, 246, 238, 218, 277, 246, 159, 16, 228, 240, 16, 228, 240, 141, 1], [0, 53, 61, 197, 76, 115, 143, 245, 94, 228, 200, 262, 63, 163, 172, 151, 113, 5, 303, 141, 1], [0, 163, 215, 101, 68, 228, 278, 262, 293, 307, 262, 150, 197, 228, 240, 73, 35, 218, 212, 246, 53, 33, 257, 140, 141, 163, 186, 175, 244, 168, 99, 113, 273, 176, 156, 113, 305, 248, 141, 1], [0, 163, 215, 101, 161, 228, 225, 110, 228, 240, 73, 230, 218, 257, 140, 212, 246, 53, 24, 141, 216, 197, 179, 228, 102, 12, 154, 237, 272, 163, 260, 197, 203, 228, 71, 92, 141, 1], [0, 288, 70, 113, 193, 99, 277, 16, 228, 240, 103, 238, 143, 245, 184, 173, 1], [0, 71, 240, 127, 183, 93, 266, 103, 173, 1], [0, 66, 157, 277, 16, 139, 38, 218, 163, 260, 53, 122, 277, 23, 157, 281, 270, 141, 218, 38, 113, 266, 246, 96, 16, 228, 240, 141, 1], [0, 292, 124, 245, 191, 1], [0, 277, 53, 214, 295, 268, 126, 149, 69, 95, 99, 245, 191, 1], [0, 143, 31, 173, 245, 238, 1], [0, 255, 113, 37, 127, 99, 228, 240, 84, 113, 245, 65, 238, 1], [0, 89, 173, 65, 174, 182, 216, 197, 210, 135, 77, 1], [0, 7, 200, 262, 173, 1], [0, 173, 91, 94, 71, 38, 285, 144, 275, 143, 173, 1], [0, 262, 173, 99, 179, 232, 246, 53, 242, 1], [0, 238, 245, 259, 302, 1], [0, 51, 191, 259, 1], [0, 301, 211, 173, 238, 1], [0, 282, 246, 113, 59, 99, 228, 238, 245, 1], [0, 213, 228, 259, 245, 238, 1], [0, 40, 88, 163, 123, 253, 213, 244, 173, 241, 259, 245, 238, 1], [0, 173, 127, 246, 75, 94, 212, 65, 53, 44, 169, 1]]\n"
     ]
    }
   ],
   "source": [
    "#Convert text into inp ids\n",
    "inp = []\n",
    "for sentence in data:\n",
    "    tokens = []\n",
    "    tokens.append(0)\n",
    "    for word in sentence.split():\n",
    "        id = word_indices[word.lower()]\n",
    "        tokens.append(id)\n",
    "    tokens.append(1)\n",
    "    \n",
    "   \n",
    "\n",
    "    inp.append(tokens)\n",
    "\n",
    "\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9468d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataset\n",
    "X =[]\n",
    "y = []\n",
    "\n",
    "for i in inp:\n",
    "    x = [0,]\n",
    "    for j in range(1,len(i)-1):\n",
    "        \n",
    "        x.append(i[j])\n",
    "        pad = []\n",
    "        for k in range(0,64-len(x)):\n",
    "            pad.append(2)\n",
    "        #Adding padding\n",
    "        fin = x+pad\n",
    "\n",
    "        #Append        \n",
    "        X.append(fin)\n",
    "        y.append(i[j+1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4299a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X,dtype=torch.int)\n",
    "y_tensor = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da6ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data and creating a dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tensor,y_tensor,train_size=0.8,shuffle=True)\n",
    "dataset = TensorDataset(X_train,y_train)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ceb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting model hyperparameters\n",
    "epochs = 50\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm.parameters(),lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b0df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . LOSS:: 4.87255537051421\n",
      "1 . LOSS:: 4.847856238484383\n",
      "2 . LOSS:: 4.822704485975779\n",
      "3 . LOSS:: 4.8036259813950615\n",
      "4 . LOSS:: 4.781799190319502\n",
      "5 . LOSS:: 4.765793140117939\n",
      "6 . LOSS:: 4.749238291612039\n",
      "7 . LOSS:: 4.735228060529782\n",
      "8 . LOSS:: 4.714361125460038\n",
      "9 . LOSS:: 4.698558622827897\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "lm.train()\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    for X_batch,y_batch in dataloader:\n",
    "        #Creating a mask\n",
    "        batch_size = X_batch.size(0)\n",
    "        seq_len = X_batch.size(1)\n",
    "        \n",
    "        #Creating a casual mask\n",
    "        casual_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()  # (seq_len, seq_len)\n",
    "        casual_mask = casual_mask.unsqueeze(0) \n",
    "        casual_mask = casual_mask.repeat(batch_size,  1, 1) \n",
    "        \n",
    "        \n",
    "        y_pred = lm(X_batch,casual_mask)[:, -1, :]\n",
    "        loss = loss_fn(y_pred,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    print(epoch,\". LOSS::\",loss_val/len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  tensor(8.1325)\n"
     ]
    }
   ],
   "source": [
    "#Training accuracy\n",
    "lm.eval()\n",
    "y_pred = lm(X_train,None)[:,-1,:]\n",
    "y_pred = torch.softmax(y_pred,dim=-1)\n",
    "\n",
    "predicted= y_pred.argmax(dim=-1)\n",
    "target = y_train.argmax(dim=-1)\n",
    "acc = (predicted==y_train).float().mean()\n",
    "\n",
    "print(\"Accuracy \",acc.item()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e08493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  6.009615212678909\n"
     ]
    }
   ],
   "source": [
    "#Testing accuracy\n",
    "lm.eval()\n",
    "y_pred = lm(X_test,None)[:,-1,:]\n",
    "y_pred = torch.softmax(y_pred,dim=-1)\n",
    "\n",
    "predicted= y_pred.argmax(dim=-1)\n",
    "\n",
    "acc = (predicted==y_test).float().mean()\n",
    "\n",
    "print(\"Accuracy \",acc.item()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f9cf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we actually generate text\n",
    "\n",
    "def generate_text(model,prompt:str,token_dict:dict,temperature:float,max_new_tokens: int)->str:\n",
    "\n",
    "    #We make an inverse dictionary\n",
    "    inv_token_dict = {v: k for k,v in token_dict.items() }\n",
    "\n",
    "    pad_token = token_dict['<PAD>']\n",
    "    \n",
    "    encoded_inp = [token_dict.get(ch.lower(),pad_token) for ch in prompt.split()]\n",
    "    encoded_inp.insert(0,0)\n",
    "    inp_ids = torch.tensor([encoded_inp],dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "    generated = inp_ids.clone()\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        logits = model(generated,None)\n",
    "\n",
    "        next_token_logits = logits[:,-1,:]/ temperature\n",
    "        probs = F.softmax(next_token_logits,dim=-1)\n",
    "        \n",
    "        #Sample from distribution\n",
    "        next_token = torch.multinomial(probs,num_samples=1)\n",
    "        generated = torch.cat([generated,next_token],dim=-1)\n",
    "        if next_token.item() == 1: #EOS\n",
    "            break\n",
    "\n",
    "\n",
    "    output_tokens = generated[0].tolist()\n",
    "    return \" \".join([inv_token_dict[tok] for tok in output_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3017ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> i applied for a credit card updating apply intermediaries shows works got cash problem month end any detail are error failure - my printed works card\n"
     ]
    }
   ],
   "source": [
    "p = str(input())\n",
    "print(generate_text(lm,p,word_indices,0.1,20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
