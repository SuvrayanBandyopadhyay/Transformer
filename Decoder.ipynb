{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dea9e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A pytorch implementation of a transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pandas as pd\n",
    "from TransformLib.Transformer import Decoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9b95583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size:int,d_model:int,n_layers:int,h:int,d_ff:int,max_seq_len:int = 512):\n",
    "        super().__init__()\n",
    "        self.decode = Decoder(vocab_size,d_model,n_layers,h,d_ff,max_seq_len)\n",
    "        self.output = nn.Linear(d_model,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,x,target_mask):\n",
    "        x = self.decode(x,None,None,target_mask)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a1ca94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<CLS>': 0, '<SEP>': 1, '<PAD>': 2, 'related': 3, 'mobile': 4, 'credit': 5, 'birth': 6, 'you': 7, 'card': 8, 'error': 9, '.': 10, 'complete': 11, 'amount': 12, 'digital': 13, 'gets': 14, 'sorry': 15, 'password': 16, 'online': 17, 'correct': 18, 'what': 19, 'name': 20, 'date': 21, 'updating': 22, 'are': 23, 'information': 24, 'out': 25, 'pos': 26, 'attached': 27, 'making': 28, 'as': 29, 'approved': 30, 'gateway': 31, 'any': 32, 'i': 33, 'settlement': 34, '?': 35, 'successful': 36, 'using': 37, 'many': 38, 'change': 39, 'when': 40, 'application': 41, 'insufficient': 42, 'recognised': 43, 'after': 44, 'reset': 45, 'works': 46, 'large': 47, 'also': 48, 'atm': 49, 'security': 50, 'applying': 51, 'ten': 52, 'personal': 53, 'cash': 54, 'shows': 55, 'things': 56, 'number': 57, 'your': 58, 'process': 59, 'facing': 60, 'has': 61, 'cascading': 62, 'want': 63, 'on': 64, 'status': 65, 'collected': 66, 'system': 67, 'activation': 68, 'entering': 69, 'please': 70, 'old': 71, 'another': 72, 'kolkata': 73, 'been': 74, 'wrong': 75, 'though': 76, 'lost': 77, 'with': 78, 'booking': 79, 'today': 80, 'allowed': 81, 'add': 82, 'soon': 83, 'shop': 84, 'deduction': 85, 'due': 86, 'debit': 87, 'emi': 88, 'transactions': 89, 'declined': 90, 'even': 91, 'from': 92, 'regarding': 93, 'copy': 94, 'but': 95, 'so': 96, 'increased': 97, 'forgot': 98, 'charge': 99, 'intermediaries': 100, 'processed': 101, 'shown': 102, 'deducted': 103, 'changed': 104, 'to': 105, 'of': 106, 'along': 107, 'updated': 108, 'send': 109, 'in': 110, 'ago': 111, 'will': 112, 'exact': 113, 'there': 114, 'opening': 115, 'registered': 116, 'fullfill': 117, 'where': 118, 'issuing': 119, 'middle': 120, 'did': 121, 'reason': 122, 'bill': 123, 'below': 124, 'documents': 125, 'whenever': 126, 'wrongly': 127, 'possible': 128, 'although': 129, 'pan': 130, 'my': 131, 'site': 132, 'by': 133, 'father': 134, 'find': 135, 'mentioned': 136, 'make': 137, 'transfer': 138, 'gave': 139, 'interest': 140, 'irctc': 141, 'complaint': 142, 'internet': 143, 'required': 144, 'time': 145, 'found': 146, 'detected': 147, 'ticket': 148, 'unable': 149, 'merchants': 150, 'bank': 151, 'closing': 152, 'one': 153, 'money': 154, 'debited': 155, 'refunded': 156, 'gone': 157, 'have': 158, 'some': 159, 'week': 160, 'till': 161, 'correctly': 162, 'instead': 163, 'shopping': 164, 'detail': 165, 'settled': 166, 'local': 167, 'all': 168, 'correcting': 169, 'use': 170, 'purchasing': 171, 'while': 172, 'generate': 173, 'reveals': 174, 'tell': 175, 'inquire': 176, 'working': 177, 'connection': 178, 'yesterday': 179, 'pending': 180, 'loan': 181, 'block': 182, 'was': 183, 'banking': 184, 'yet': 185, 'last': 186, 'refund': 187, 'into': 188, 'linked': 189, 'user': 190, 'correction': 191, 'because': 192, 'go': 193, 'credited': 194, 'corrected': 195, 'without': 196, 'days': 197, 'that': 198, 'during': 199, 'loss': 200, 'increasing': 201, '3': 202, 'month': 203, 'get': 204, 'actual': 205, 'poor': 206, '7': 207, 'otp': 208, 'something': 209, 'criteria': 210, 'linking': 211, 'at': 212, 'calculated': 213, 'it': 214, 'or': 215, 'claim': 216, 'mechanical': 217, 'able': 218, 'pin': 219, 'got': 220, '\"': 221, '-': 222, 'authentication': 223, 'and': 224, 'denied': 225, 'is': 226, 'mall': 227, 'education': 228, 'machine': 229, 'joint': 230, 'mother': 231, 'the': 232, 'details': 233, 'provided': 234, 'incorrect': 235, 'deduced': 236, 'printed': 237, 'day': 238, 'made': 239, 'problem': 240, 'again': 241, 'cause': 242, 'limits': 243, 'conformed': 244, 'transaction': 245, 'this': 246, 'address': 247, 'given': 248, 'through': 249, 'why': 250, '/': 251, 'completed': 252, 'payment': 253, 'could': 254, 'not': 255, 'balance': 256, 'for': 257, 'am': 258, 'valid': 259, 'opened': 260, 'applied': 261, 'unsuccessful': 262, 'can': 263, 'withdrawn': 264, 'necessary': 265, 'be': 266, 'sufficient': 267, 'side': 268, 'do': 269, 'received': 270, 'document': 271, 'daily': 272, 'end': 273, 'entered': 274, 'which': 275, 'apply': 276, 'unique': 277, 'changing': 278, 'via': 279, 'now': 280, 'involving': 281, 'message': 282, 'failure': 283, 'delays': 284, 'fund': 285, 'a': 286, 'withdrew': 287, 'size': 288, 'issued': 289, 'disbursal': 290, ',': 291, 'started': 292, 'paid': 293, 'passbook': 294, 'account': 295, 'sms': 296, 'an': 297, 'check': 298, 'book': 299, 'form': 300, 'failed': 301, 'how': 302, 'done': 303, 'withdrawing': 304, 'previously': 305, 'other': 306, 'new': 307}\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "data = df['text'].tolist()\n",
    "\n",
    "wordlist = []\n",
    "\n",
    "for i in data:\n",
    "    words = i.split()\n",
    "    for w in words:\n",
    "        wordlist.append(w.lower())\n",
    "\n",
    "word_indices = dict({\"<CLS>\":0,\"<SEP>\":1,\"<PAD>\":2})\n",
    " \n",
    "index = 3\n",
    "for w in set(wordlist):\n",
    "    word_indices.update({w:index})\n",
    "    index+=1\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "87818e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_indices)\n",
    "d_model = 32\n",
    "n_layers = 8\n",
    "h = 8\n",
    "d_ff = 128\n",
    "max_len = 64\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "lm = LanguageModel(vocab_size,d_model,n_layers,h,d_ff,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27f8c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 33, 261, 257, 286, 5, 8, 186, 203, 95, 33, 121, 255, 204, 198, 153, 161, 280, 10, 129, 33, 117, 168, 232, 210, 144, 257, 51, 5, 8, 10, 1], [0, 33, 260, 286, 307, 295, 110, 58, 151, 95, 40, 33, 220, 131, 294, 33, 146, 198, 131, 120, 20, 226, 127, 237, 64, 214, 10, 1], [0, 33, 261, 257, 87, 215, 5, 8, 40, 33, 260, 131, 295, 110, 58, 151, 186, 203, 95, 33, 121, 255, 204, 32, 161, 280, 10, 1], [0, 161, 280, 33, 121, 255, 204, 32, 87, 215, 5, 8, 275, 33, 261, 186, 203, 110, 58, 151, 10, 1], [0, 33, 139, 297, 41, 257, 211, 277, 57, 78, 131, 151, 295, 186, 203, 95, 161, 280, 214, 226, 255, 108, 10, 1], [0, 33, 63, 105, 39, 232, 4, 57, 198, 183, 305, 189, 78, 131, 295, 224, 63, 105, 82, 131, 307, 57, 10, 1], [0, 33, 77, 131, 87, 215, 5, 8, 179, 10, 96, 33, 63, 105, 182, 131, 87, 215, 5, 8, 10, 232, 3, 24, 226, 248, 124, 10, 1], [0, 70, 182, 131, 87, 215, 5, 8, 192, 33, 77, 131, 87, 215, 5, 8, 179, 10, 168, 232, 3, 24, 226, 248, 124, 10, 1], [0, 33, 139, 297, 41, 257, 211, 131, 4, 57, 78, 131, 295, 186, 160, 95, 161, 280, 33, 269, 255, 204, 32, 24, 93, 198, 10, 1], [0, 33, 139, 297, 41, 257, 211, 131, 4, 57, 78, 131, 295, 186, 203, 95, 33, 121, 255, 204, 32, 296, 3, 105, 131, 295, 64, 116, 232, 57, 10, 1], [0, 33, 261, 257, 286, 307, 295, 110, 58, 151, 95, 40, 33, 220, 131, 294, 33, 146, 198, 232, 20, 106, 131, 134, 226, 127, 237, 64, 214, 10, 1], [0, 33, 258, 255, 218, 105, 137, 253, 249, 131, 87, 215, 5, 8, 10, 297, 9, 282, 183, 102, 198, 221, 246, 8, 226, 255, 43, 221, 10, 1], [0, 199, 253, 249, 87, 8, 214, 55, 253, 283, 10, 297, 9, 282, 183, 102, 198, 221, 246, 8, 226, 255, 43, 221, 10, 1], [0, 33, 261, 257, 286, 307, 295, 110, 58, 151, 95, 40, 33, 220, 131, 294, 33, 146, 198, 131, 21, 106, 6, 226, 127, 237, 64, 232, 294, 10, 1], [0, 33, 63, 105, 82, 131, 130, 8, 57, 105, 131, 295, 96, 198, 33, 263, 137, 47, 245, 133, 37, 214, 10, 1], [0, 33, 137, 286, 253, 249, 131, 87, 8, 212, 286, 164, 227, 10, 232, 253, 65, 55, 283, 106, 253, 95, 256, 226, 236, 92, 131, 295, 10, 1], [0, 232, 253, 226, 255, 252, 95, 256, 226, 236, 92, 131, 295, 40, 33, 137, 286, 253, 212, 286, 164, 227, 10, 1], [0, 33, 139, 297, 41, 257, 22, 131, 53, 233, 64, 131, 295, 95, 214, 226, 255, 303, 161, 280, 10, 33, 48, 27, 232, 265, 125, 78, 232, 41, 10, 1], [0, 33, 261, 257, 115, 286, 230, 295, 78, 131, 231, 186, 203, 95, 40, 33, 220, 131, 295, 165, 33, 146, 198, 214, 226, 286, 53, 295, 10, 1], [0, 33, 261, 257, 115, 286, 53, 295, 186, 203, 95, 40, 33, 220, 131, 295, 165, 33, 146, 198, 214, 226, 286, 230, 295, 78, 131, 134, 10, 1], [0, 33, 139, 297, 41, 257, 152, 131, 295, 64, 58, 151, 186, 160, 10, 33, 48, 136, 232, 259, 122, 257, 152, 232, 295, 163, 214, 226, 255, 303, 161, 280, 10, 1], [0, 33, 261, 257, 143, 184, 68, 64, 131, 295, 186, 160, 95, 161, 280, 33, 269, 255, 204, 32, 24, 93, 198, 10, 33, 234, 168, 232, 144, 24, 78, 131, 41, 300, 10, 1], [0, 33, 98, 131, 143, 184, 16, 224, 63, 105, 45, 95, 33, 258, 255, 218, 105, 45, 232, 16, 10, 1], [0, 33, 98, 131, 143, 184, 190, 20, 224, 63, 105, 45, 214, 95, 33, 183, 149, 105, 269, 198, 10, 1], [0, 33, 137, 245, 249, 87, 8, 257, 304, 154, 92, 49, 10, 232, 245, 226, 255, 252, 95, 154, 226, 236, 92, 131, 295, 10, 1], [0, 33, 261, 257, 143, 184, 68, 64, 131, 295, 52, 238, 111, 95, 33, 269, 255, 204, 131, 190, 20, 224, 16, 161, 280, 10, 1], [0, 33, 261, 257, 278, 131, 53, 295, 105, 286, 230, 295, 78, 131, 134, 186, 203, 95, 161, 280, 214, 226, 255, 104, 10, 1], [0, 33, 139, 297, 41, 257, 169, 131, 167, 247, 106, 131, 295, 95, 214, 226, 255, 195, 161, 179, 10, 1], [0, 33, 170, 131, 87, 8, 257, 304, 154, 92, 131, 295, 10, 232, 245, 226, 255, 252, 86, 105, 159, 122, 95, 154, 226, 236, 92, 131, 295, 10, 1], [0, 33, 170, 131, 87, 8, 257, 28, 253, 64, 297, 17, 132, 10, 154, 226, 236, 92, 131, 295, 95, 253, 65, 226, 102, 29, 262, 10, 1], [0, 33, 239, 286, 245, 64, 297, 17, 132, 257, 171, 209, 10, 232, 253, 65, 226, 255, 36, 95, 154, 226, 236, 92, 131, 295, 10, 1], [0, 33, 63, 105, 276, 257, 228, 181, 95, 232, 277, 57, 189, 105, 131, 295, 226, 255, 18, 10, 33, 261, 257, 169, 232, 277, 57, 95, 214, 226, 255, 195, 185, 10, 33, 48, 27, 153, 94, 106, 131, 18, 277, 57, 107, 78, 131, 41, 10, 1], [0, 33, 139, 297, 41, 257, 211, 286, 307, 4, 57, 105, 131, 295, 29, 131, 71, 189, 4, 57, 226, 255, 177, 95, 161, 280, 214, 226, 255, 303, 10, 86, 105, 246, 33, 258, 60, 38, 240, 10, 1], [0, 33, 139, 297, 41, 257, 201, 131, 272, 245, 243, 10, 33, 48, 27, 286, 94, 106, 131, 130, 8, 78, 232, 41, 300, 95, 214, 226, 255, 97, 161, 280, 10, 70, 269, 214, 29, 83, 29, 128, 10, 1], [0, 33, 63, 105, 138, 154, 92, 131, 295, 105, 72, 295, 279, 131, 87, 8, 95, 255, 218, 105, 11, 232, 138, 10, 214, 55, 297, 9, 10, 1], [0, 33, 63, 105, 138, 154, 92, 131, 295, 105, 72, 295, 279, 131, 87, 8, 10, 95, 208, 226, 255, 109, 133, 151, 105, 131, 116, 4, 57, 96, 198, 33, 263, 137, 232, 245, 10, 1], [0, 33, 239, 286, 245, 78, 131, 87, 8, 257, 171, 56, 92, 286, 84, 110, 73, 10, 232, 245, 226, 255, 36, 95, 154, 226, 236, 92, 131, 295, 10, 1], [0, 245, 65, 226, 166, 92, 131, 268, 95, 183, 255, 270, 64, 232, 306, 273, 1], [0, 232, 245, 212, 153, 273, 226, 166, 95, 114, 23, 284, 110, 34, 212, 232, 306, 273, 1], [0, 200, 110, 232, 256, 110, 295, 86, 105, 67, 283, 172, 17, 245, 1], [0, 33, 239, 286, 245, 64, 141, 37, 131, 87, 8, 257, 148, 79, 10, 79, 226, 255, 244, 91, 154, 226, 264, 92, 131, 295, 10, 1], [0, 33, 261, 257, 88, 64, 297, 17, 164, 132, 37, 131, 5, 8, 10, 88, 226, 292, 64, 131, 295, 95, 214, 226, 255, 102, 64, 232, 164, 132, 10, 1], [0, 199, 304, 154, 92, 295, 37, 87, 8, 92, 286, 49, 291, 214, 183, 102, 198, 198, 232, 219, 7, 274, 226, 235, 129, 33, 274, 232, 18, 219, 10, 1], [0, 232, 247, 136, 64, 131, 295, 226, 255, 18, 10, 33, 261, 257, 247, 191, 186, 160, 95, 214, 226, 255, 195, 161, 80, 10, 1], [0, 232, 219, 7, 274, 226, 75, 282, 183, 102, 40, 33, 183, 304, 154, 92, 49, 37, 87, 8, 224, 69, 232, 18, 219, 10, 1], [0, 33, 63, 105, 39, 131, 87, 8, 219, 95, 255, 218, 105, 269, 96, 212, 32, 49, 229, 10, 1], [0, 33, 258, 255, 218, 105, 39, 131, 87, 8, 50, 219, 37, 32, 49, 229, 129, 33, 274, 232, 18, 24, 144, 257, 198, 59, 10, 1], [0, 302, 269, 33, 135, 25, 250, 286, 253, 215, 187, 245, 301, 1], [0, 253, 65, 226, 30, 133, 214, 61, 255, 74, 270, 1], [0, 199, 304, 154, 92, 49, 37, 87, 291, 232, 245, 226, 301, 95, 154, 226, 236, 92, 131, 295, 92, 131, 295, 10, 1], [0, 255, 218, 105, 137, 32, 17, 245, 133, 131, 87, 8, 129, 33, 274, 168, 232, 24, 162, 10, 1], [0, 33, 261, 257, 22, 131, 130, 8, 224, 277, 8, 57, 105, 131, 295, 186, 160, 95, 214, 226, 255, 108, 161, 80, 10, 33, 48, 27, 286, 94, 106, 232, 144, 125, 78, 232, 41, 300, 10, 1], [0, 33, 261, 257, 169, 131, 20, 64, 131, 295, 186, 203, 95, 161, 80, 214, 226, 255, 195, 10, 86, 105, 246, 131, 306, 46, 23, 180, 118, 33, 158, 105, 170, 131, 151, 233, 10, 1], [0, 33, 261, 257, 298, 299, 186, 203, 95, 161, 80, 33, 269, 255, 204, 32, 10, 1], [0, 286, 298, 299, 226, 289, 64, 131, 295, 95, 33, 269, 255, 261, 257, 32, 153, 10, 86, 105, 246, 99, 257, 119, 298, 299, 226, 236, 92, 131, 295, 10, 1], [0, 255, 218, 105, 138, 154, 105, 72, 295, 133, 131, 87, 8, 10, 1], [0, 33, 261, 257, 232, 307, 87, 8, 186, 203, 95, 121, 255, 204, 32, 161, 280, 129, 33, 234, 168, 232, 144, 271, 212, 232, 145, 106, 51, 10, 1], [0, 33, 220, 131, 307, 87, 8, 179, 95, 33, 258, 255, 218, 105, 173, 131, 50, 219, 212, 32, 49, 10, 1], [0, 131, 295, 55, 232, 154, 61, 74, 155, 95, 214, 61, 255, 74, 293, 212, 232, 132, 10, 1], [0, 131, 253, 254, 255, 266, 101, 291, 76, 33, 158, 267, 256, 110, 131, 295, 10, 1], [0, 126, 33, 193, 257, 253, 291, 114, 226, 223, 283, 10, 1], [0, 114, 183, 286, 253, 31, 283, 224, 154, 61, 74, 155, 10, 1], [0, 40, 112, 232, 154, 266, 194, 241, 35, 1], [0, 253, 225, 86, 105, 42, 285, 282, 291, 76, 33, 158, 267, 256, 10, 1], [0, 33, 260, 286, 307, 295, 110, 58, 151, 95, 40, 33, 220, 131, 294, 33, 146, 198, 131, 120, 20, 226, 127, 237, 64, 214, 10, 1], [0, 33, 261, 257, 87, 215, 5, 8, 40, 33, 260, 131, 295, 110, 58, 151, 186, 203, 95, 33, 121, 255, 204, 32, 161, 280, 10, 1], [0, 161, 280, 33, 121, 255, 204, 32, 87, 215, 5, 8, 275, 33, 261, 186, 203, 110, 58, 151, 10, 1], [0, 33, 139, 297, 41, 257, 211, 277, 57, 78, 131, 151, 295, 186, 203, 95, 161, 280, 214, 226, 255, 108, 10, 1], [0, 245, 255, 166, 1], [0, 301, 49, 245, 1], [0, 110, 49, 295, 14, 155, 196, 205, 290, 106, 54, 1], [0, 110, 253, 89, 281, 100, 291, 232, 253, 105, 183, 255, 239, 105, 150, 1], [0, 245, 65, 226, 166, 92, 131, 268, 95, 183, 255, 270, 64, 232, 306, 273, 1], [0, 232, 245, 212, 153, 273, 226, 166, 95, 114, 23, 284, 110, 34, 212, 232, 306, 273, 1], [0, 200, 110, 232, 256, 110, 295, 86, 105, 67, 283, 172, 17, 245, 1], [0, 114, 61, 74, 62, 283, 110, 131, 295, 224, 232, 140, 61, 48, 74, 213, 64, 232, 75, 12, 1], [0, 154, 61, 74, 103, 92, 131, 295, 1], [0, 33, 258, 255, 218, 105, 39, 131, 87, 8, 50, 219, 37, 32, 49, 229, 129, 33, 274, 232, 18, 24, 144, 257, 198, 59, 10, 1], [0, 199, 304, 154, 92, 49, 37, 87, 291, 232, 245, 226, 301, 95, 154, 226, 236, 92, 131, 295, 92, 131, 295, 10, 1], [0, 255, 218, 105, 137, 32, 17, 245, 133, 131, 87, 8, 129, 33, 274, 168, 232, 24, 162, 10, 1], [0, 33, 261, 257, 22, 131, 130, 8, 224, 277, 8, 57, 105, 131, 295, 186, 160, 95, 214, 226, 255, 108, 161, 80, 10, 33, 48, 27, 286, 94, 106, 232, 144, 125, 78, 232, 41, 300, 10, 1], [0, 33, 261, 257, 169, 131, 20, 64, 131, 295, 186, 203, 95, 161, 80, 214, 226, 255, 195, 10, 86, 105, 246, 131, 306, 46, 23, 180, 118, 33, 158, 105, 170, 131, 151, 233, 10, 1], [0, 176, 188, 232, 85, 106, 154, 92, 131, 295, 199, 301, 17, 245, 251, 253, 1], [0, 151, 295, 65, 174, 103, 12, 199, 253, 1], [0, 287, 159, 154, 92, 49, 291, 95, 33, 158, 255, 66, 154, 192, 159, 217, 240, 10, 95, 291, 232, 12, 226, 147, 92, 131, 295, 10, 1], [0, 142, 93, 245, 283, 1], [0, 154, 255, 156, 91, 44, 202, 222, 207, 197, 106, 245, 283, 1], [0, 17, 123, 253, 245, 301, 1], [0, 175, 232, 113, 65, 106, 131, 295, 29, 232, 245, 61, 301, 1], [0, 13, 253, 61, 157, 75, 86, 105, 206, 143, 178, 1], [0, 262, 87, 8, 253, 1], [0, 253, 90, 133, 151, 291, 9, 282, 172, 17, 253, 1], [0, 8, 253, 106, 246, 288, 226, 255, 81, 1], [0, 301, 245, 187, 216, 1], [0, 26, 283, 187, 1], [0, 15, 58, 253, 301, 1], [0, 19, 226, 232, 242, 106, 131, 301, 245, 1], [0, 250, 131, 187, 245, 301, 1], [0, 302, 269, 33, 135, 25, 250, 286, 253, 215, 187, 245, 301, 1], [0, 253, 65, 226, 30, 133, 214, 61, 255, 74, 270, 1]]\n"
     ]
    }
   ],
   "source": [
    "#Convert text into inp ids\n",
    "inp = []\n",
    "for sentence in data:\n",
    "    tokens = []\n",
    "    tokens.append(0)\n",
    "    for word in sentence.split():\n",
    "        id = word_indices[word.lower()]\n",
    "        tokens.append(id)\n",
    "    tokens.append(1)\n",
    "    \n",
    "   \n",
    "\n",
    "    inp.append(tokens)\n",
    "\n",
    "\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9468d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataset\n",
    "X =[]\n",
    "y = []\n",
    "\n",
    "for i in inp:\n",
    "    x = [0,]\n",
    "    for j in range(1,len(i)-1):\n",
    "        \n",
    "        x.append(i[j])\n",
    "        pad = []\n",
    "        for k in range(0,64-len(x)):\n",
    "            pad.append(2)\n",
    "        #Adding padding\n",
    "        fin = x+pad\n",
    "\n",
    "        #Append        \n",
    "        X.append(fin)\n",
    "        y.append(i[j+1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4299a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X,dtype=torch.int)\n",
    "y_tensor = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1da6ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data and creating a dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tensor,y_tensor,train_size=0.8,shuffle=True)\n",
    "dataset = TensorDataset(X_train,y_train)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "86ceb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting model hyperparameters\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm.parameters(),lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . LOSS:: 5.1012709851448355\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "lm.train()\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    for X_batch,y_batch in dataloader:\n",
    "        #Creating a mask\n",
    "        batch_size = X_batch.size(0)\n",
    "        seq_len = X_batch.size(1)\n",
    "        \n",
    "        #Creating a casual mask\n",
    "        casual_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()  # (seq_len, seq_len)\n",
    "        casual_mask = casual_mask.unsqueeze(0) \n",
    "        casual_mask = casual_mask.repeat(batch_size,  1, 1) \n",
    "        \n",
    "        \n",
    "        y_pred = lm(X_batch,casual_mask)[:, -1, :]\n",
    "        loss = loss_fn(y_pred,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    print(epoch,\". LOSS::\",loss_val/len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e08493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lm.eval()\n",
    "y_pred = lm(X_test,None)[:,-1,:]\n",
    "y_pred = torch.softmax(y_pred,dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bd7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted= y_pred.argmax(dim=-1)\n",
    "target = y_test.argmax(dim=-1)\n",
    "acc = (predicted==target).float().mean()\n",
    "\n",
    "print(\"Accuracy \",acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we actually generate text\n",
    "\n",
    "def generate_text(model,prompt:str,token_dict:dict,temperature:float,max_new_tokens: int)->str:\n",
    "\n",
    "    #We make an inverse dictionary\n",
    "    inv_token_dict = {v: k for k,v in token_dict.items() }\n",
    "\n",
    "    pad_token = token_dict['<PAD>']\n",
    "    \n",
    "    encoded_inp = [token_dict.get(ch.lower(),pad_token) for ch in prompt.split()]\n",
    "    encoded_inp.insert(0,0)\n",
    "    inp_ids = torch.tensor([encoded_inp],dtype=torch.long)\n",
    "\n",
    "    model.eval()\n",
    "    generated = inp_ids.clone()\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        logits = model(generated,None)\n",
    "\n",
    "        next_token_logits = logits[:,-1,:]/ temperature\n",
    "        probs = F.softmax(next_token_logits,dim=-1)\n",
    "        \n",
    "        #Sample from distribution\n",
    "        next_token = torch.multinomial(probs,num_samples=1)\n",
    "        generated = torch.cat([generated,next_token],dim=-1)\n",
    "        if next_token.item() == 1: #EOS\n",
    "            break\n",
    "\n",
    "\n",
    "    output_tokens = generated[0].tolist()\n",
    "    return \" \".join([inv_token_dict[tok] for tok in output_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CLS> credit till fullfill copy intermediaries has mentioned not updated or get have information any - allowed birth . what how intermediaries\n"
     ]
    }
   ],
   "source": [
    "p = str(input())\n",
    "print(generate_text(lm,p,word_indices,0.1,20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
